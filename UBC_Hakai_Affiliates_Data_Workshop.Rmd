---
title: "Data Management Workshop for the Pelagic Ecosystems Lab and Hakai Affiliates at UBC"
author: "Brett Johnson & Julian Gan"
date: "31/01/2020"
output: bookdown::pdf_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE,echo = TRUE)
library(tidyverse)
library(lubridate)
library(here)
```

# Pre-workshop material

Did you successfully install and set up all the tools necessary for this workshop?

* Download Slack
* Update or install R
* Update or install R-Studio
* Update or install packages
* Get an account for the Hakai Data Portal from data@hakai.org
* Get Git and GitHub set up

See [README](https://github.com/Br-Johnson/2019-01-21-UBCIOF/blob/gh-pages/README.md)

## Objectives

The objectives of this workshop are:

1. Become familiar with tools to manage and analyze data efficiently
  + Write code in R-Studio
  + Use `tidyverse`, `dplyr`, `ggplot2` and `tidyr` R packages to analyze your data
  + Use Git and GitHub version control and a changelog 

2. Produce well-documented tidy data-sets that have excellent provenance
  + Data sets contain a change log with a version history of what has changed and the steps used to process data
  + All variables in the dataset are defined in a data dictionary
  + Laboratory, and analytical methods are thoroughly described
  
3. Create reproducible analyses
  + You can easily re-run your code and analysis when you get new data
  + Others can easily find your code and understand the steps you took to analyze your data
  + Raw data are available

# Day 1 AM: Intro to R for Data Management

## Vectors and data types

------------

> ### Learning Objectives
>
> * Inspect the content of vectors and manipulate their content.

------------

A vector is the most common and basic data type in R, and is pretty much
the workhorse of R. A vector is composed by a series of values, which can be
either numbers or characters. We can assign a series of values to a vector using
the `c()` function. For example we can create a vector of animal weights and assign
it to a new object `weight_g`:

```{r, purl=FALSE}
weight_g <- c(50, 60, 65, 82)
weight_g
```

```{r, purl=FALSE}
animals <- c("mouse", "rat", "dog")
animals
```


An **atomic vector** is the simplest R **data type** and is a linear vector of a single type. Above, we saw 
2 of the 6 main **atomic vector** types  that R
uses: `"character"` and `"numeric"` (or `"double"`). These are the basic building blocks that
all R objects are built from. The other 4 **atomic vector** types are:

* `"logical"` for `TRUE` and `FALSE` (the boolean data type)
* `"integer"` for integer numbers (e.g., `2L`, the `L` indicates to R that it's an integer)
* `"complex"` to represent complex numbers with real and imaginary parts (e.g.,
  `1 + 4i`) and that's all we're going to say about them
* `"raw"` for bitstreams that we won't discuss further

You can check the type of your vector using the `typeof()` function and inputting your vector as the argument.

Vectors are one of the many **data structures** that R uses. Other important
ones are lists (`list`), matrices (`matrix`), data frames (`data.frame`),
factors (`factor`) and arrays (`array`).


## Starting with data

------------

> ### Learning Objectives
>
> * Describe what a data frame is.
> * Load external data from a .csv file into a data frame.
> * Summarize the contents of a data frame.

------------

### Presentation of the Survey Data

```{r, echo=FALSE, purl=TRUE}
### Presentation of the survey data
```

We are going to use the R function `download.file()` to download the CSV file
that contains the survey data from figshare, and we will use `read_csv()` to
load into memory the content of the CSV file as an object of class `data.frame`. 
Inside the download.file command, the first entry is a character string with the source URL 
("https://ndownloader.figshare.com/files/2292169"). This source URL downloads a CSV file from 
figshare. The text after the comma ("data/portal_data_joined.csv") is the destination of the 
file on your local machine. You'll need to have a folder on your machine called "read_data" where 
you'll download the file. So this command downloads a file from figshare, names it 
"portal_data_joined.csv," and adds it to a preexisting folder named "data."

```{r, eval=FALSE, purl=TRUE}
library(tidyverse)
library(here)

download.file(url="https://ndownloader.figshare.com/files/2292169",
              destfile = "read_data/portal_data_joined.csv")
```

You are now ready to load the data:

```{r, eval=TRUE,  purl=FALSE}
surveys <- read_csv(here("read_data", "portal_data_joined.csv"))
```

This statement doesn't produce any output because assignments don't display anything. If we want to check that our data has been
loaded, we can see the contents of the data frame by typing its name: `surveys`.

Wow... that was a lot of output. At least it means the data loaded
properly. Let's check the top (the first 6 lines) of this data frame using the
function `head()`:

```{r, results='show', purl=FALSE}
head(surveys)
## Try also
## View(surveys)
```

### What are data frames?

Data frames are the _de facto_ data structure for most tabular data, and what we
use for statistics and plotting.

A data frame can be created by hand, but most commonly they are generated by the
functions `read_csv()` or `read.table()`; in other words, when importing
spreadsheets from your hard drive (or the web).

A data frame is the representation of data in the format of a table where the
columns are vectors that all have the same length. Because columns are
vectors, each column must contain a single type of data (e.g., characters, integers,
factors). For example, here is a figure depicting a data frame comprising a
numeric, a character, and a logical vector.

![](./img/data-frame.png)


We can see this when inspecting the <b>str</b>ucture of a data frame
with the function `str()`:

```{r, purl=FALSE}
str(surveys)
```

### Inspecting `data.frame` Objects

We already saw how the functions `head()` and `str()` can be useful to check the
content and the structure of a data frame. Here is a non-exhaustive list of
functions to get a sense of the content/structure of the data. Let's try them out!

* Size:
    * `dim(surveys)` - returns a vector with the number of rows in the first element,
          and the number of columns as the second element (the **dim**ensions of
          the object)
    * `nrow(surveys)` - returns the number of rows
    * `ncol(surveys)` - returns the number of columns

* Summary:
    * `str(surveys)` - structure of the object and information about the class, length and
	   content of  each column
    * `summary(surveys)` - summary statistics for each column

Note: most of these functions are "generic", they can be used on other types of
objects besides `data.frame`.

### Indexing and subsetting data frames

```{r, echo=FALSE, purl=TRUE}

## Indexing and subsetting data frames
```


Our survey data frame has rows and columns (it has 2 dimensions), if we want to
extract some specific data from it, we need to specify the "coordinates" we
want from it. Row numbers come first, followed by column numbers. However, note
that different ways of specifying these coordinates lead to results with
different classes.


```{r, purl=FALSE}
# first element in the first column of the data frame (as a vector)
surveys[1, 1]   
# first element in the 6th column (as a vector)
surveys[1, 6]   
# first column of the data frame (as a vector)
surveys[, 1]    
# first column of the data frame (as a data.frame)
surveys[1]      
# first three elements in the 7th column (as a vector)
surveys[1:3, 7] 
# the 3rd row of the data frame (as a data.frame)
surveys[3, ]    
# equivalent to head_surveys <- head(surveys)
head_surveys <- surveys[1:6, ] 
```

`:` is a special function that creates numeric vectors of integers in increasing
or decreasing order, test `1:10` and `10:1` for instance.

You can also exclude certain indices of a data frame using the "`-`" sign:

```{r, purl=FALSE}
surveys[, -1]          # The whole data frame, except the first column
surveys[-c(7:34786), ] # Equivalent to head(surveys)
```

Data frames can be subset by calling indices (as shown previously), but also by calling their column names directly:

```{r, eval = FALSE, purl=FALSE}
surveys["species_id"]       # Result is a data.frame
surveys[, "species_id"]     # Result is a vector
surveys[["species_id"]]     # Result is a vector
surveys$species_id          # Result is a vector
```

In RStudio, you can use the autocompletion feature to get the full and correct names of the columns.


## Data Manipulation

------------

> ### Learning Objectives
>
> * Describe the purpose of the **`dplyr`** and **`tidyr`** packages.
> * Select certain columns in a data frame with the **`dplyr`** function `select`.
> * Select certain rows in a data frame according to filtering conditions with the **`dplyr`** function `filter` .
> * Link the output of one **`dplyr`** function to the input of another function with the 'pipe' operator `%>%`.
> * Add new columns to a data frame that are functions of existing columns with `mutate`.
> * Use `group_by`, and `summarize` to split a data frame into groups of observations, apply a summary statistics for each group, and then combine the results.
> * Describe the concept of a wide and a long table format and for which purpose those formats are useful.
> * Describe what key-value pairs are.
> * Reshape a data frame from long to wide format and back with the `spread` and `gather` commands from the **`tidyr`** package.

------------

### **`dplyr`** and **`tidyr`**

Bracket subsetting is handy, but it can be cumbersome and difficult to read,
especially for complicated operations. Enter **`dplyr`**. **`dplyr`** is a package for
making tabular data manipulation easier. It pairs nicely with **`tidyr`** which enables you to swiftly convert between different data formats for plotting and analysis.

Packages in R are basically sets of additional functions that let you do more
stuff. The functions we've been using so far, like `str()` or `data.frame()`,
come built into R; packages give you access to more of them. Before you use a
package for the first time you need to install it on your machine, and then you
should import it in every subsequent R session when you need it. You should
already have installed the **`tidyverse`** package. This is an
"umbrella-package" that installs several packages useful for data analysis which
work together well such as **`tidyr`**, **`dplyr`**, **`ggplot2`**, **`tibble`**, etc.

To load the package, type:

```{r, message = FALSE, purl = FALSE}
## load the tidyverse packages, incl. dplyr
library("tidyverse")
```


The package **`dplyr`** provides easy tools for the most common data manipulation
tasks. It is built to work directly with data frames.

The package **`tidyr`** addresses the common problem of wanting to reshape your data for plotting and use by different R functions. Sometimes we want data sets where we have one row per measurement. Sometimes we want a data frame where each measurement type has its own column, and rows are instead more aggregated groups - like plots or aquaria. Moving back and forth between these formats is nontrivial, and **`tidyr`** gives you tools for this and more sophisticated  data manipulation.

To learn more about **`dplyr`** and **`tidyr`** after the workshop, you may want to check out this
[handy data transformation with **`dplyr`** cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) and this [one about **`tidyr`**](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf).


```{r, results = 'hide', purl = FALSE}
surveys <- read_csv(here("read_data", "portal_data_joined.csv"))

## inspect the data
str(surveys)

## preview the data
# View(surveys)
```

We're going to learn some of the most common **`dplyr`** functions:

- `select()`: subset columns
- `filter()`: subset rows on conditions
- `mutate()`: create new columns by using information from other columns
- `group_by()` : creates groups based on categorical data in a column
- `summarize()`: create summary statisitcs on grouped data
- `arrange()`: sort results
- `count()`: count discrete values

### Selecting columns and filtering rows

To select columns of a data frame, use `select()`. The first argument
to this function is the data frame (`surveys`), and the subsequent
arguments are the columns to keep.

```{r, results = 'hide', purl = FALSE}
select(surveys, plot_id, species_id, weight)
```

To select all columns *except* certain ones, put a "-" in front of
the variable to exclude it.

```{r, results = 'hide', purl = FALSE}
select(surveys, -record_id, -species_id)
```

This will select all the variables in `surveys` except `record_id`
and `species_id`.

To choose rows based on a specific criteria, use `filter()`:

```{r, purl = FALSE}
filter(surveys, year == 1995)
```

### Pipes

What if you want to select and filter at the same time? There are three
ways to do this: use intermediate steps, nested functions, or pipes.

With intermediate steps, you create a temporary data frame and use
that as input to the next function, like this:

```{r, purl = FALSE}
surveys2 <- filter(surveys, weight < 5)
surveys_sml <- select(surveys2, species_id, sex, weight)
```

This is readable, but can clutter up your workspace with lots of objects that you have to name individually. With multiple steps, that can be hard to keep track of.

The a sometimes better option, *pipes*, are a recent addition to R. Pipes let you take
the output of one function and send it directly to the next, which is useful
when you need to do many things to the same dataset.  Pipes in R look like
`%>%` and are made available via the **`magrittr`** package, installed automatically
with **`dplyr`**. If you use RStudio, you can type the pipe with <kbd>Ctrl</kbd>
+ <kbd>Shift</kbd> + <kbd>M</kbd> if you have a PC or <kbd>Cmd</kbd> + 
<kbd>Shift</kbd> + <kbd>M</kbd> if you have a Mac.

```{r, purl = FALSE}
surveys %>%
  filter(weight < 5) %>%
  select(species_id, sex, weight)
```

In the above code, we use the pipe to send the `surveys` dataset first through
`filter()` to keep rows where `weight` is less than 5, then through `select()`
to keep only the `species_id`, `sex`, and `weight` columns. Since `%>%` takes
the object on its left and passes it as the first argument to the function on
its right, we don't need to explicitly include the data frame as an argument
to the `filter()` and `select()` functions any more.

Some may find it helpful to read the pipe like the word "then". For instance,
in the above example, we took the data frame `surveys`, *then* we `filter`ed
for rows with `weight < 5`, *then* we `select`ed columns `species_id`, `sex`,
and `weight`. The **`dplyr`** functions by themselves are somewhat simple,
but by combining them into linear workflows with the pipe, we can accomplish
more complex manipulations of data frames.

If we want to create a new object with this smaller version of the data, we
can assign it a new name:

```{r, purl = FALSE}
surveys_sml <- surveys %>%
  filter(weight < 5) %>%
  select(species_id, sex, weight)

surveys_sml
```

Note that the final data frame is the leftmost part of this expression.

> ### Challenge {.challenge}
>
>  Using pipes, subset the `surveys` data to include animals collected before
>  1995 and retain only the columns `year`, `sex`, and `weight`.
> 
> 
> ```{r, answer=TRUE, eval=FALSE, purl=FALSE}
> surveys %>%
>     filter(year < 1995) %>%
>     select(year, sex, weight)
> ```

```{r, eval=FALSE, purl=TRUE, echo=FALSE}
## Pipes Challenge:
##  Using pipes, subset the data to include animals collected
##  before 1995, and retain the columns `year`, `sex`, and `weight.`
```

### Mutate

Frequently you'll want to create new columns based on the values in existing
columns, for example to do unit conversions, or to find the ratio of values in two
columns. For this we'll use `mutate()`.

To create a new column of weight in kg:

```{r, purl = FALSE}
surveys %>%
  mutate(weight_kg = weight / 1000)
```

You can also create a second new column based on the first new column within the same call of `mutate()`:

```{r, purl = FALSE}
surveys %>%
  mutate(weight_kg = weight / 1000,
         weight_kg2 = weight_kg * 2)
```

If this runs off your screen and you just want to see the first few rows, you
can use a pipe to view the `head()` of the data. (Pipes work with non-**`dplyr`**
functions, too, as long as the **`dplyr`** or `magrittr` package is loaded).

```{r, purl = FALSE}
surveys %>%
  mutate(weight_kg = weight / 1000) %>%
  head()
```

The first few rows of the output are full of `NA`s, so if we wanted to remove
those we could insert a `filter()` in the chain:

```{r, purl = FALSE}
surveys %>%
  drop_na(weight) %>%
  mutate(weight_kg = weight / 1000) %>%
  head()
```

`drop_na()` is a function that determines whether something is an `NA`, and will drop the whole row if there is an NA in the column you specified (weight in this case). Using `drop_na()` with no column named will look for an NA in any column and drop the whole row if there are any NAs

> ### Challenge {.challenge}
>
>  Create a new data frame from the `surveys` data that meets the following
>  criteria: contains only the `species_id` column and a new column called
>  `hindfoot_half` containing values that are half the `hindfoot_length` values.
>  In this `hindfoot_half` column, there are no `NA`s and all values are less
>  than 30.
>
>  **Hint**: think about how the commands should be ordered to produce this data frame!
> 
> ```{r, answer=TRUE, eval=FALSE, purl=FALSE}
> surveys_hindfoot_half <- surveys %>%
>     drop_na(hindfoot_length) %>%
>     mutate(hindfoot_half = hindfoot_length / 2) %>%
>     filter(hindfoot_half < 30) %>%
>     select(species_id, hindfoot_half)
> ```
 

```{r, eval=FALSE, purl=TRUE, echo=FALSE}
## Mutate Challenge:
##  Create a new data frame from the `surveys` data that meets the following
##  criteria: contains only the `species_id` column and a column that
##  contains values that are half the `hindfoot_length` values (e.g. a
##  new column `hindfoot_half`). In this `hindfoot_half` column, there are
##  no NA values and all values are < 30.

##  Hint: think about how the commands should be ordered to produce this data frame!
```

### The `summarize()` function

`group_by()` is often used together with `summarize()`, which collapses each
group into a single-row summary of that group.  `group_by()` takes as arguments
the column names that contain the **categorical** variables for which you want
to calculate the summary statistics. So to compute the mean `weight` by sex:

```{r, purl = FALSE}
surveys %>%
  group_by(sex) %>%
  summarize(mean_weight = mean(weight, na.rm = TRUE))
```

You may also have noticed that the output from these calls doesn't run off the
screen anymore. It's one of the advantages of `tbl_df` over data frame.

You can also group by multiple columns:

```{r, purl = FALSE}
surveys %>%
  group_by(sex, species_id) %>%
  summarize(mean_weight = mean(weight, na.rm = TRUE))
```

When grouping both by `sex` and `species_id`, the last few rows are for animals
that escaped before their sex and body weights could be determined. You may notice
that the last column does not contain `NA` but `NaN` (which refers to "Not a
Number"). To avoid this, we can remove the missing values for weight before we
attempt to calculate the summary statistics on weight. Because the missing
values are removed first, we can omit `na.rm = TRUE` when computing the mean:

```{r, purl = FALSE}
surveys %>%
  drop_na(weight) %>%
  group_by(sex, species_id) %>%
  summarize(mean_weight = mean(weight))
```

Here, again, the output from these calls doesn't run off the screen
anymore. If you want to display more data, you can use the `print()` function
at the end of your chain with the argument `n` specifying the number of rows to
display:

```{r, purl = FALSE}
surveys %>%
  filter(!is.na(weight)) %>%
  group_by(sex, species_id) %>%
  summarize(mean_weight = mean(weight)) %>%
  print(n = 15)
```

Once the data are grouped, you can also summarize multiple variables at the same
time (and not necessarily on the same variable). For instance, we could add a
column indicating the minimum weight for each species for each sex:

```{r, purl = FALSE}
surveys %>%
  filter(!is.na(weight)) %>%
  group_by(sex, species_id) %>%
  summarize(mean_weight = mean(weight),
            min_weight = min(weight))
```

It is sometimes useful to rearrange the result of a query to inspect the values. For instance, we can sort on `min_weight` to put the lighter species first:


```{r, purl = FALSE}
surveys %>%
  filter(!is.na(weight)) %>%
  group_by(sex, species_id) %>%
  summarize(mean_weight = mean(weight),
            min_weight = min(weight)) %>%
  arrange(min_weight)
```

To sort in descending order, we need to add the `desc()` function. If we want to sort the results by decreasing order of mean weight:

```{r, purl = FALSE}
surveys %>%
  filter(!is.na(weight)) %>%
  group_by(sex, species_id) %>%
  summarize(mean_weight = mean(weight),
            min_weight = min(weight)) %>%
  arrange(desc(mean_weight))
```


### Counting

When working with data, we often want to know the number of observations found
for each factor or combination of factors. For this task, **`dplyr`** provides
`count()`. For example, if we wanted to count the number of rows of data for
each sex, we would do:

```{r, purl = FALSE}
surveys %>%
    count(sex) 
```

The `count()` function is shorthand for something we've already seen: grouping by a variable, and summarizing it by counting the number of observations in that group. In other words, `surveys %>% count()` is equivalent to:  

```{r, purl = FALSE}
surveys %>%
    group_by(sex) %>%
    summarise(count = n())
```

For convenience, `count()` provides the `sort` argument:  

```{r, purl = FALSE}
surveys %>%
    count(sex, sort = TRUE) 
```

> ### Challenge {.challenge}
>
> 1. How many animals were caught in each `plot_type` surveyed?
>
> ```{r, answer=TRUE, purl=FALSE}
> surveys %>%
>     count(plot_type) 
> ```
>
> 2. Use `group_by()` and `summarize()` to find the mean, min, and max hindfoot
> length for each species (using `species_id`). Also add the number of
> observations (hint: see `?n`).
>
> ```{r, answer=TRUE, purl=FALSE}
> surveys %>%
>     filter(!is.na(hindfoot_length)) %>%
>     group_by(species_id) %>%
>     summarize(
>         mean_hindfoot_length = mean(hindfoot_length),
>         min_hindfoot_length = min(hindfoot_length),
>         max_hindfoot_length = max(hindfoot_length),
>         n = n()
>     )
> ```
>
> 3. What was the heaviest animal measured in each year? Return the columns `year`,
> `genus`, `species_id`, and `weight`.
>
> ```{r, answer=TRUE, purl=FALSE}
> surveys %>%
>     filter(!is.na(weight)) %>%
>     group_by(year) %>%
>     filter(weight == max(weight)) %>%
>     select(year, genus, species, weight) %>%
>     arrange(year)
> ```


```{r, eval=FALSE, purl=TRUE, echo=FALSE}
## Count Challenges:
##  1. How many animals were caught in each `plot_type` surveyed?

##  2. Use `group_by()` and `summarize()` to find the mean, min, and max
## hindfoot length for each species (using `species_id`). Also add the number of
## observations (hint: see `?n`).

##  3. What was the heaviest animal measured in each year? Return the
##  columns `year`, `genus`, `species_id`, and `weight`.
```

## Exporting data

Now that you have learned how to use **`dplyr`** to extract information from
or summarize your raw data, you may want to export these new processed data sets to share them with your collaborators or for archival.

Never write to `read_data/`

Similar to the `read_csv()` function used for reading CSV files into R, there is
a `write_csv()` function that generates CSV files from data frames.

```{r}
write_csv(surveys_sml, here("write_data", "surveys_sml.csv"))
```

* Knit your document

* Commit, Pull, Push

Questions?


## Principles of tidy data

Tidy data

1. Each variable has its own column
2. Each observation has its own row
3. Each value must have its own cell
4. Each type of observational unit forms a table

Sometimes your you want to spread the observations of on variable across multiple columns.

In `surveys` , the rows of `surveys` contain the values of variables associated
with each record (the unit), values such the weight or sex of each animal 
associated with each record. What if instead of comparing records, we 
wanted to compare the different mean weight of each species between plots? (Ignoring `plot_type` for simplicity).

We'd need to create a new table where each row (the unit) is comprised of values of variables associated with each plot. In practical terms this means the values
of the species in `genus` would become the names of column variables and the cells would contain the values of the mean weight observed on each plot.

Having created a new table, it is therefore straightforward to explore the 
relationship between the weight of different species within, and between, the
plots. The key point here is that we are still following a tidy data structure,
but we have **reshaped** the data according to the observations of interest:
average species weight per plot instead of recordings per date.

The opposite transformation would be to transform column names into values of
a variable.

We can do both these of transformations with two `tidyr` functions, `pivot_wider()`
and `pivot_longer()`.

### Widening (aka. Spreading)

`pivot_wider()` makes a dataset wider by increasing the number of columns and decreasing the number of rows.
It takes three principal arguments:

1. the data
2. the *names_from* column variable whose values will become the new column names.
3. the *values_from* column whose values will fill the new column variables.

Further arguments include `values_fill` which, if set, fills in missing values with the value provided.

Let's use `pivot_wider()` to transform surveys to find the mean weight of each species in each plot over the entire survey period. We use `filter()`, `group_by()`, and `summarize()` to filter our observartions and variables of interest, and create a new variable for the `mean_weight`. We use the pipe as before too.

```{r, purl=FALSE}
surveys_gw <- surveys %>%
  filter(!is.na(weight)) %>%
  group_by(genus, plot_id) %>%
  summarize(mean_weight = mean(weight))

str(surveys_gw)
```

This yields `surveys_gw` where the observations for each plot are spread across
multiple rows, 196 observations of 3 variables. 
Using `pivot_wider()` to names from `genus` with values from `mean_weight` this becomes
24 observations of 11 variables, one row for each plot. We again use pipes:

```{r, purl=FALSE}
surveys_wide <- surveys_gw %>%
  pivot_wider(names_from = genus, values_from = mean_weight)

str(surveys_wide)
```

![](img/spread_data_R.png)

We could now plot comparisons between the weight of species in different plots, 
although we may wish to fill in the missing values first.

```{r, purl=FALSE}
surveys_gw %>%
  spread(genus, mean_weight, fill = 0) %>%
  head()
```

### Lengthening (aka. Gathering)

The opposing situation could occur if we had been provided with data in the
form of `surveys_spread`, where the genus names are column names, but we 
wish to treat them as values of a genus variable instead.

In this situation we are gathering the column names and turning them into a
pair of new variables. One variable represents the column names as values, and
the other variable contains the values previously associated with the column names.

`pivot_longer()` takes four principal arguments:

1. the data
2. the *names_to* column variable we wish to create from column names.
3. the *values_to* column variable we wish to create and fill with values 
associated with the new column.
4. the names of the columns we use to fill the key variable (or to drop).

To recreate `surveys_gw` from `surveys_wide` we would create a key called
`genus` and value called `mean_weight` and use all columns except `plot_id` for
the key variable. Here we drop `plot_id` column with a minus sign.

```{r, purl=FALSE}
surveys_long <- surveys_wide %>%
  pivot_longer(names_to = "genus", values_to = "mean_weight", -plot_id)

str(surveys_long)
```

![](img/gather_data_R.png)

Note that now the `NA` genera are included in the re-lengthened format. WIdening and then lengthening can be a useful way to balance out a dataset so every
replicate has the same composition.

We could also have used a specification for what columns to include. This can be
useful if you have a large number of identifying columns, and it's easier to
specify what to gather than what to leave alone. And if the columns are in a
row, we don't even need to list them all out - just use the `:` operator!

```{r, purl=FALSE}
surveys_wide %>%
  pivot_longer(names_to = "genus", values_to = "mean_weight", Baiomys:Spermophilus) %>%
  head()
```

> ### Challenge {.challenge}
>
> 1. Widen the `surveys` data frame with `year` as columns, `plot_id` 
>   as rows, and the
>   number of genera per plot as the values. You will need to summarize before
>   reshaping, and use the function `n_distinct()` to get the number of unique
>   genera within a particular chunk of data. It's a powerful function! See
>   `?n_distinct` for more.
> 
> ```{r, answer=TRUE, purl=FALSE}
> rich_time <- surveys %>%
>   group_by(plot_id, year) %>%
>   summarize(n_genera = n_distinct(genus)) %>%
>   pivot_wider(names_from = year, values_from = n_genera)
> 
> head(rich_time)
> ```
>
> 2. Now take that data frame and `pivot_longer()` it again, so each row is a unique
>    `plot_id` by `year` combination.
>
> ```{r, answer=TRUE, purl=FALSE}
> rich_time %>%
>   pivot_longer(names_to = "year", values_to = "n_genera", -plot_id)
> ```
>
> 3. The `surveys` data set has
>    two measurement columns: `hindfoot_length` and `weight`.  This makes it
>    difficult to do things like look at the relationship between mean values of
>    each measurement per year in different plot types. Let's walk through a
>    common solution for this type of problem. First, use `pivot_longer()` to create a
>     dataset where we have a key column called `measurement` and a
>    `value` column that takes on the value of either `hindfoot_length` or
>    `weight`. *Hint*: You'll need to specify which columns are being gathered.
>
> ```{r, answer=TRUE, purl=FALSE}
> surveys_long <- surveys %>%
>   pivot_longer(names_to = "measurement", values_to = "value", cols = c(hindfoot_length, weight))
> ```
>
> 4. With this new data set, calculate the average of each
>    `measurement` in each `year` for each different `plot_type`. Then
>    `pivot_wider()` them into a data set with a column for `hindfoot_length` and
>    `weight`. *Hint*: You only need to specify the key and value
>    columns for `pivot_wider()`.
>
> ```{r, answer=TRUE, purl=FALSE}
> surveys_long %>%
>   group_by(year, measurement, plot_type) %>%
>   summarize(mean_value = mean(value, na.rm=TRUE)) %>%
>   pivot_wider(names_from = measurement, values_from = mean_value)
> ```

```{r, eval=FALSE, purl=TRUE, echo=FALSE}
## Reshaping challenges

## 1. Make a wide data frame with `year` as columns, `plot_id`` as rows, and where the values are the number of genera per plot. You will need to summarize before reshaping, and use the function `n_distinct` to get the number of unique genera within a chunk of data. It's a powerful function! See `?n_distinct` for more.

## 2. Now take that data frame, and make it long again, so each row is a unique `plot_id` `year` combination

## 3. The `surveys` data set is not truly wide or long because there are two columns of measurement - `hindfoot_length` and `weight`.  This makes it difficult to do things like look at the relationship between mean values of each measurement per year in different plot types. Let's walk through a common solution for this type of problem. First, use `pivot_wider` to create a truly long dataset where we have a key column called `measurement` and a `value` column that takes on the value of either `hindfoot_length` or `weight`. Hint: You'll need to specify which columns are being gathered.

## 4. With this new truly long data set, calculate the average of each `measurement` in each `year` for each different `plot_type`. Then `pivot_wider` them into a wide data set with a column for `hindfoot_length` and `weight`. Hint: Remember, you only need to specify the key and value columns for `pivot_wider`.

```


# Day 1 PM: Intermediate Topics in R

## R Markdown



## Data Quality Reports

Often, as we go through a data analysis we are checking the quality of the data as we go. You might, for example:
* Count the number of NAs in specific columns
* Check measurements for outliers
* Check sampling locations accuracy (are your sites on land?)
* Check all date and time formats are valid

It's often difficult to know at the end of an analysis project whether data quality has accidentally been compromised. It's also difficult to know what checks of the data have been done. Therefore it's a great idea to write tests into your data analysis as you go, such that if something becomes a problem, your diagnostic plots or tables will alert you!

```{r}
library(obistools)

names <- c("Abra alva", "Buccinum fusiforme", "Buccinum fusiforme", "Buccinum fusiforme", "hlqsdkf")
match_taxa(names)





```


## Data Visualization Including Maps

### Plotting with **`ggplot2`**

**`ggplot2`** is a plotting package that makes it simple to create complex plots
from data in a data frame. It provides a more programmatic interface for
specifying what variables to plot, how they are displayed, and general visual properties. 

**`ggplot2`** functions like data in the 'long' format, i.e., a column for every variable,
and a row for every observation. Well-structured data will save you lots of time
when making figures with **`ggplot2`**

ggplot graphics are built step by step by adding new elements. Adding layers in
this fashion allows for extensive flexibility and customization of plots.

To build a ggplot, we will use the following basic template that can be used for different types of plots:

```
ggplot(data = <DATA>, mapping = aes(<MAPPINGS>)) +  <GEOM_FUNCTION>()
```

- use the `ggplot()` function and bind the plot to a specific data frame using the
      `data` argument

```{r, eval=FALSE, purl=FALSE}
ggplot(data = surveys_complete)
```

- define a mapping (using the aesthetic (`aes`) function), by selecting the variables to be plotted and specifying how to present them in the graph, e.g. as x/y positions or characteristics such as size, shape, color, etc.

```{r, eval=FALSE, purl=FALSE}
ggplot(data = surveys_complete, mapping = aes(x = weight, y = hindfoot_length))
```

- add 'geoms' â€“ graphical representations of the data in the plot (points, lines, bars). **`ggplot2`** offers many different geoms; we will use some 
  common ones today, including:
  
      * `geom_point()` for scatter plots, dot plots, etc.
      * `geom_boxplot()` for, well, boxplots!
      * `geom_line()` for trend lines, time series, etc.  


**Notes**

- Anything you put in the `ggplot()` function can be seen by any geom layers
  that you add (i.e., these are universal plot settings). This includes the x- and
  y-axis mapping you set up in `aes()`.
- You can also specify mappings for a given geom independently of the
  mappings defined globally in the `ggplot()` function.
- The `+` sign used to add new layers must be placed at the end of the line containing
the *previous* layer. If, instead, the `+` sign is added at the beginning of the line
containing the new layer, **`ggplot2`** will not add the new layer and will return an 
error message.

### **`ggplot2`** themes

In addition to `theme_bw()`, which changes the plot background to white, **`ggplot2`**
comes with several other themes which can be useful to quickly change the look
of your visualization. The complete list of themes is available
at <http://docs.ggplot2.org/current/ggtheme.html>. `theme_minimal()` and
`theme_light()` are popular, and `theme_void()` can be useful as a starting
point to create a new hand-crafted theme.

The
[ggthemes](https://jrnold.github.io/ggthemes/reference/index.html) package
provides a wide variety of options (including an Excel 2003 theme).
The [**`ggplot2`** extensions website](https://www.ggplot2-exts.org) provides a list
of packages that extend the capabilities of **`ggplot2`**, including additional
themes.

#### Faceting

* `facet_wrap()`
* `facet_grid()`

#### Arranging Multiple Plots

`cowplot()`

```{r}
library(tidyverse)

# The tidyverse library has a data frame object called mpg, it's about cars.
# Check it out
mpg

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

# What about this other column class? Maybe we want to see what type of car it is too

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))

# It would be nice to see a trend line

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +
  geom_smooth(mapping = aes(x = displ, y = hwy))

# The default smoothing line is a loess model, which looks funny here, lets use a linear model

# It would be nice to see a trend line

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class)) +
  geom_smooth(mapping = aes(x = displ, y = hwy), method = lm)

ggsave(here("figs", "mpg.png"))
```

### Site Maps

Section Contributors:

* Dr. Daniel Okamoto
* Jenn Burt 

A common task amongst most field researchers is the need to make a basic site map to describe the location of your sampling. Using Arc GIS is the most common way to produce maps at Hakai, but sometimes a simple solution that could be implemented in R is desired. 

For a high resolution map with the resolution needed to see detailed coastline features you can use the following code and shapefile. To get this to work on your computer, download the shape file and put it in a R Studio project sub-folder called data.


```{r, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)
###################################################### #
###  Script to make a BC map                       ### #
###  Author:  D.K. Okamoto (modified by Jenn Burt) ### #
###################################################### #

# Libraries needed to run this code
library(raster)
library(maps) 
library(mapdata)
library(maptools)
library(mapproj)
library(rgeos)
library(rgdal)
library(ggplot2)
library(ggsn)
library(tidyverse)
library(here)
```

#### High Resolution Maps

The high resolution map used here requires that you download a set of ESRI shape files from this book's GitHUB repository. Those files can be downloaded from the `2_Shapefile` folder [here](https://github.com/HakaiInstitute/hakai_guide_to_r/tree/master/data). Put the `2_Shapefile` folder into the data folder of your R-Studio project.

This script assumes you are using the `here()` package in conjunction with R-Studio projects to obviate setting your working directory. 

```{r}
############## Make a map with sites ################## 
########## Using high resolution shapefile ################

BC.shp <- readOGR(here("read_data","2_Shapefile", "COAST_TEST2.shp"))

### chose the lat/long extent you want to show
Ncalvert <- extent(-128.18, -127.94, 51.61, 51.78)

### crop your shapefile polygons to the extent defined
# takes a moment to run (patience grasshopper)
BC.shp2 <- crop(BC.shp,Ncalvert)

### project and fortify (i.e. turn into a dataframe)
BC.df <- fortify(BC.shp2)

# (IF DESIRED) Load .csv file with your specific study site lat/longs
# this file is a dataframe with 4 columns: site_name, otterOcc(Y or N), lat, long  
# EXPT
# sites <- read.csv("/Users/jennb/Dropbox/Simple_BC_map/EXPTsites.csv", header = T)

# Jenn graph 
# here is where you can see the styles of north arrow (scroll to
# bottom): http://oswaldosantos.github.io/ggsn/ 
# the high resolution shape file works well at this scale 
# as it gives lots of the coastline detail
ggplot() + theme_bw() +
  geom_polygon(
    data = BC.df,
    aes(x = long, y = lat, group = group),
    colour = "black",
    size = 0.1,
    fill = 'grey95'
  ) +
  coord_cartesian(xlim = c(-128.17, -127.95),
                  ylim = c(51.63, 51.772)) +
  scalebar(
    BC.df,
    dist = 3,
    dist_unit = "km",
    st.size = 4,
    height = 0.01,
    transform = TRUE,
    model = 'WGS84',
    anchor = c(x = -127.96, y = 51.63)
  ) +
  north(
    data = BC.df,
    scale = 0.1,
    symbol = 3,
    anchor = c(x = -128.15, y = 51.775)
  ) +
  theme(
    panel.grid.minor = element_line(colour = NA),
    panel.grid.major = element_line(colour = NA),
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10)
  )
```

```{r}
### if you want to make a larger Central coast map, just change the extent selected
CCoast <- extent(-128.48, -127.9, 51.5, 52.1)
# crop the map to the new extent
CC.shp2 <- crop(BC.shp,CCoast)
# fortify
CC.df <- fortify(CC.shp2)

# Jenn graph
fig1 <- ggplot() + theme_bw() +
  geom_polygon(
    data = CC.df,
    aes(x = long, y = lat, group = group),
    colour = "black",
    size = 0.1,
    fill = 'grey85'
  ) +
  coord_cartesian(xlim = c(-128.5, -127.95),
                  ylim = c(51.63, 52.05)) +
  scale_x_continuous(breaks = c(-128.4, -128.2, -128.0)) +
  scalebar(
    CC.df,
    dist = 5,
    dist_unit = "km",
    st.size = 3.5,
    height = 0.014,
    transform = TRUE,
    model = 'WGS84',
    anchor = c(x = -128.33, y = 51.64)
  ) +
  north(
    data = CC.df,
    scale = 0.07,
    symbol = 3,
    anchor = c(x = -128.465, y = 52.056)
  ) +
  theme(
    panel.grid.minor = element_line(colour = NA),
    panel.grid.major = element_line(colour = NA),
    axis.title.y = element_blank(),
    axis.title.x = element_blank(),
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10),
    legend.position = "none"
  )
fig1

# I use this code to export a nice PDF file of specific dimensions.
cairo_pdf("Fig1.pdf", width=4, height=5)
print(fig1)
dev.off()
```


#### Medium Resolution PBS Mapping Package

The Pacific Biological Station in Nanaimo has put together a mapping package that contains some medium resolution files of the Pacific Coast.

```{r}
############## Make a map with the sites ################## #
############# Using DFO coastline data file ################

#this is lower resolution than the shapefile above

library(PBSmapping)

## Plot the map
data(nepacLLhigh)       # DFO BC Coastline data - high resolution
plotMap(
  nepacLLhigh,
  xlim = c(-128.52, -127.93),
  ylim = c(51.56, 52.07),
  col = "grey90",
  bg = "white",
  tckMinor = 0,
  xlab = "",
  ylab = "",
  lwd = 0.5
)
box()

# add a scale bar
map.scale(
  x = -128.455,
  y = 51.61,
  ratio = FALSE,
  relwidth = 0.2
)
```

#### Low Resolution Pacific Coast Maps

For lower resolution maps to represent larger scales you can use the maps from the `maps` and `mapdata` packages.

```{r}
############## Pacific Coast Map ##################
################################################### #

# create a data file to make a basemap
# this database has a lower resolution (which is fine for large scale map)
m <- map_data("world", c("usa", "Canada"))

# this database has a way higher resolution
d <- map_data("worldHires", c("Canada", "usa", "Mexico"))

# make a basic map, all one colour
# play around with xlim and ylim to change the extent
ggplot() + geom_polygon(data = d, aes(x=long, y = lat, group = group)) + theme_bw()+
  coord_map("conic", lat0 = 18, xlim=c(210, 237), ylim=c(46,62))
```

```{r}
ggplot() +
  geom_polygon(
    data = subset(m, region == "Canada"),
    aes(x = long, y = lat, group = group),
    fill = "grey65",
    colour = "black",
    size = .1
  ) + theme_bw() +
  geom_polygon(
    data = subset(m, region == "USA"),
    aes(x = long, y = lat, group = group),
    fill = "white",
    colour = "black",
    size = .1
  ) +
  coord_map(
    "conic",
    lat0 = 18,
    xlim = c(195, 238),
    ylim = c(30, 62.5)
  ) +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(colour = "grey"),
    #change "grey" to NA to remove
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )
```

```{r}
# playing with extent and colour
ggplot() +
  geom_polygon(data = subset(d, region == "Canada"), aes(x = long, y = lat, group = group)) +
  geom_polygon(data = subset(d, region == "USA"),
               aes(x = long, y = lat, group = group),
               fill = "darkgrey") +
  coord_map(xlim = c(195, 240), ylim = c(45, 65)) +
  theme_bw()
```

```{r}
# here you can see that if you use the other dataframe "d" the resolution is much higher. 
# this is good for smaller chunks of the BC coast, but less good for a PNW map
ggplot() +
  geom_polygon(
    data = subset(d, region == "Canada"),
    aes(x = long, y = lat, group = group),
    fill = "darkgrey",
    colour = "black",
    size = .1
  ) + theme_bw() +
  geom_polygon(
    data = subset(d, region == "USA"),
    aes(x = long, y = lat, group = group),
    fill = "white",
    colour = "black",
    size = .1
  ) +
  coord_map(
    "conic",
    lat0 = 18,
    xlim = c(195, 240),
    ylim = c(45, 61)
  ) +
  theme(
    panel.grid.minor = element_line(colour = NA),
    panel.grid.major = element_line(colour = NA),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )
```


## Dates and Times in R

## Project-oriented workflows with R Studio and GitHub

## Collaborative Analysis Development Workflows










